{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training for Flood Detection\n",
    "\n",
    "This notebook is designed to train a convolutional neural network (CNN) for flood detection using SAR images. The training process includes data loading, preprocessing, augmentation, and model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idées sur entrainement à explorer\n",
    "\n",
    "\n",
    "Calibration radiométrique (DN → σ⁰) – Si ce n’est pas déjà fait, il faut convertir les valeurs brutes du TIFF (DN en amplitude) en valeurs physiquement calibrées. Typiquement, on applique la formule de calibration avec la LUT fournie pour obtenir l’intensité σ⁰ de chaque pixel (souvent exprimée en décibels). Cette étape met toutes les images sur une échelle comparable. Dans certains cas, on peut se contenter de travailler en « β⁰ non corrigé de l’angle », mais pour la classification d’inondation, le σ⁰ en dB est préférable afin que par exemple un sol sec à 33° et le même sol à 45° d’incidence aient des valeurs comparables après normalisation angulaire.\n",
    "\n",
    "\n",
    "Correction du bruit thermique – Les produits Sentinel‑1 contiennent des profils de bruit de fond (thermal noise) estimés, notamment pour les extrémités de swath où le signal utile est faible. Il est recommandé de soustraire ce bruit de fond (fourni dans les annotations sous forme de LUT de bruit en range/azimut) avant ou après la calibration​\n",
    "CERWEB.IFREMER.FR. Cela remet à zéro le niveau de référence des pixels qui n’ont pratiquement aucun signal rétrodiffusé, évitant de confondre du bruit avec un faible retour réel (important pour ne pas classer l’océan ou des zones réellement sans signal comme de l’eau peu profonde). Concrètement, on calcule σ⁰_calibré = (DN² / A²) – σ⁰_bruit, en veillant à ne pas obtenir de valeurs négatives (les pixels où le bruit dépasse le signal utile peuvent être mis à NaN ou à 0).\n",
    "\n",
    "\n",
    "Filtrage du speckle – Comme discuté en section 5, il est souvent bénéfique d’appliquer un filtre de speckle modéré sur l’image intensité (exemple : un filtre de Lee 3×3 ou 5×5). Pour une IA, cela peut aider à se concentrer sur les structures larges (plaques d’eau, berges, etc.) plutôt que sur le grain. Toutefois, un excès de lissage risque de supprimer de petites mares ou des détails fins utiles à la décision. Une bonne pratique est d’adapter la force du filtre à l’échelle du phénomène d’intérêt : pour des inondations s’étalant sur des dizaines de pixels, un filtre 3×3 réduit suffisamment le bruit sans effacer les zones étroites. À l’inverse, si l’architecture de l’IA intègre déjà des mécanismes pour traiter le bruit (par ex. un CNN avec de la pooling pourrait lisser implicitement), on peut choisir un filtrage plus léger. L’objectif est que le niveau de speckle résiduel ne génère pas de confusion dans la phase d’apprentissage (par exemple, éviter qu’un pixel d’eau brillamment bruité soit appris comme “non-inondé”).\n",
    "\n",
    "\n",
    "Masquage des zones non pertinentes – Cette étape dépend du contexte, mais pour de la détection d’inondation on souhaite souvent masquer certains pixels avant la classification automatique :\n",
    "Bordures et valeurs invalides : les zones en dehors de l’emprise utile (bord noir de l’image TIFF s’il y en a), ou les pixels où la calibration n’est pas définie (quelques lignes en début/fin d’image parfois). Ces pixels doivent être masqués (valeur nodata) pour ne pas induire l’IA en erreur.\n",
    "Océan et eaux permanentes : si l’objectif est de détecter les inondations terrestres, il est judicieux de masquer l’océan et éventuellement les grands plans d’eau permanents (lacs, cours d’eau habituels). En effet, ceux-ci apparaissent également sombres comme des inondations, mais ne sont pas du « terrain inondé nouvellement ». On peut utiliser une masque d’eau statique (ex. à partir de la couche ESA WorldCover ou d’une classification antérieure Sentinel‑2) pour exclure ces zones de l’analyse ou au moins informer le modèle (certains modèles concatènent ce masque en entrée supplémentaire).\n",
    "Zones d’ombre radar ou à incidence extrême : En terrain montagneux, les images SAR comportent des ombres (zones que le radar n’a pas pu éclairer, derrière une colline par exemple) et des zones de layover (chevauchement où l’écho de la pente arrive en même temps que celui de la vallée). Ces pixels peuvent apparaître sombres (ombre) ou brillants (fausses superpositions) de manière trompeuse. Il peut être utile de les repérer via un modèle numérique de terrain et de les masquer ou au moins de ne pas les considérer comme candidats à l’inondation (une ombre radar n’est pas de l’eau, même si c’est sombre). De même, les tout premiers pixels proches (incidence faible) ou les pixels en bout de swath (incidence très élevée) peuvent avoir des caractéristiques différentes (bruit plus fort, projection plus incertaine) – on peut choisir d’ignorer par prudence, par ex., les 1–2% des pixels les plus en bord d’image.\n",
    "\n",
    "\n",
    "Changement d’échelle des valeurs (normalisation) – Pour faciliter l’apprentissage de l’IA, on applique en général une normalisation des canaux en entrée. Si l’on travaille en dB, on peut par exemple recentrer les valeurs autour de –20 dB et les diviser par 10, ou appliquer un min-max pour les ramener dans [0,1] ou [–1,1]. L’important est que les distributions de valeurs en entrée du réseau soient à peu près homogènes entre les différentes images d’entraînement, afin que le modèle ne soit pas perturbé par des décalages (par exemple une image globale plus brillante suite à une pluie uniforme sur la zone – ce cas peut être partiellement compensé par une normalisation locale ou globale). La normalisation peut aussi impliquer de passer en échelle logarithmique si ce n’est pas déjà fait (beaucoup de modèles préfèrent travailler avec les valeurs en dB plutôt que linéaires, car la distribution en dB est plus gaussienne).\n",
    "\n",
    "\n",
    "Extraction de caractéristiques additionnelles – Bien que le réseau de deep learning puisse apprendre directement sur les canaux VV et VH bruts (après calibrations et filtrages ci-dessus), on enrichit parfois l’entrée avec des features dérivées pour guider l’IA. Dans le cas de deux polarisation, une caractéristique courante est le ratio polarimétrique VH/VV (ou VH–VV en dB) par pixel. Cette ratio met en évidence les différences de comportement polarimétrique indépendamment de l’intensité absolue. Par exemple, une surface inondée aura à la fois VV et VH faibles, donc un ratio VH/VV ~ 1 (0 dB) en tendance, tandis qu’une végétation aura VH plus faible que VV (ratio < 1). D’autres caractéristiques possibles : des textures (calculées via matrices de co-occurrence GLCM sur l’image dB) pour détecter le voisinage homogène de l’eau vs l’hétérogénéité du sol végétalisé – toutefois, dans les approches CNN modernes, c’est généralement le réseau lui-même qui déduira ces textures via ses filtres convolutifs. Si l’on utilise un modèle de classification non convolutif, ajouter des indicateurs de texture ou des statistiques locales peut être bénéfique. On pourrait également injecter des données auxiliaires : par ex. l’altitude du terrain (MNT) pour aider à savoir si une zone sombre dans une vallée très basse est plus susceptible d’être de l’eau qu’une zone sombre en flanc de montagne (ombre). En résumé, toute information qui peut aider à différencier l’eau du non-eau de manière cohérente peut être ajoutée comme couche supplémentaire au modèle.\n",
    "\n",
    "\n",
    "Division en ensembles et augmentation – Pour l’apprentissage supervisé, il faut préparer les patchs d’entraînement ou mosaïques, en s’assurant d’un bon équilibre de classes (inondé vs non) et en appliquant éventuellement de l’augmentation de données (rotations, ajouts de bruit, etc.) pour rendre le modèle robuste. Ce point dépasse le simple prétraitement d’une image individuelle, mais fait partie intégrante du pipeline de préparation des données avant IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def setup_path():\n",
    "    \"\"\"\n",
    "    Configure les chemins d'accès pour permettre l'import des modules du projet.\n",
    "    Ajoute le répertoire racine du projet au sys.path pour que les imports \n",
    "    de modules comme 'src' fonctionnent correctement, peu importe d'où le script est exécuté.\n",
    "    \n",
    "    Retourne:\n",
    "        bool: True si la configuration a réussi, False sinon\n",
    "    \"\"\"\n",
    "    # Dans un notebook, __file__ n'est pas défini, on utilise getcwd() à la place\n",
    "    try:\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        current_dir = os.getcwd()\n",
    "        \n",
    "    print(f\"Répertoire courant: {current_dir}\")\n",
    "    \n",
    "    # Essayer de trouver le répertoire racine du projet (jusqu'à 3 niveaux au-dessus)\n",
    "    root_dir = current_dir\n",
    "    max_levels = 3\n",
    "    \n",
    "    for _ in range(max_levels):\n",
    "        # Vérifier si nous sommes à la racine du projet\n",
    "        if os.path.exists(os.path.join(root_dir, 'src')):\n",
    "            # Nous avons trouvé le répertoire racine\n",
    "            if root_dir not in sys.path:\n",
    "                sys.path.append(root_dir)\n",
    "                print(f\"Ajout de {root_dir} au sys.path\")\n",
    "            \n",
    "            # Vérifier si l'import fonctionne maintenant\n",
    "            try:\n",
    "                # Tenter d'importer un module pour vérifier\n",
    "                importlib.import_module('src')\n",
    "                print(\"✅ Configuration des chemins réussie.\")\n",
    "                return True\n",
    "            except ImportError as e:\n",
    "                print(f\"❌ Échec de l'import après ajout au sys.path: {e}\")\n",
    "        \n",
    "        # Remonter d'un niveau\n",
    "        parent_dir = os.path.dirname(root_dir)\n",
    "        if parent_dir == root_dir:  # Nous sommes déjà à la racine du système de fichiers\n",
    "            break\n",
    "        root_dir = parent_dir\n",
    "    \n",
    "    # Si nous arrivons ici, nous n'avons pas trouvé le répertoire racine\n",
    "    # Essayer une dernière approche en ajoutant le parent direct\n",
    "    last_resort = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "    if last_resort not in sys.path:\n",
    "        sys.path.append(last_resort)\n",
    "        print(f\"Tentative de dernier recours: ajout de {last_resort} au sys.path\")\n",
    "    \n",
    "    # Vérifier une dernière fois\n",
    "    try:\n",
    "        importlib.import_module('src')\n",
    "        print(\"✅ Configuration des chemins réussie (dernier recours).\")\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"❌ Échec de la configuration des chemins: {e}\")\n",
    "        print(\"Structure de projet attendue:\")\n",
    "        print(\"project_root/\")\n",
    "        print(\"├── src/\")\n",
    "        print(\"│   ├── data/\")\n",
    "        print(\"│   ├── models/\")\n",
    "        print(\"│   └── ...\")\n",
    "        print(\"└── notebooks/\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répertoire courant: c:\\Users\\mokht\\Desktop\\PDS\\flood_dataset\\flood-detection-cnn\\notebooks\n",
      "Ajout de c:\\Users\\mokht\\Desktop\\PDS\\flood_dataset\\flood-detection-cnn au sys.path\n",
      "✅ Configuration des chemins réussie.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from src.data.dataloader import SARDataset\n",
    "from src.models.cnn import FloodDetectionCNN, train_flood_detection_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'entraînement du modèle de détection d'inondations...\n",
      "Chargement du dataset depuis sar_dataset.pkl...\n",
      "Chargement du dataset depuis sar_dataset.pkl...\n",
      "Dataset chargé: 3331 échantillons\n",
      "Chargement du dataset depuis sar_dataset.pkl...\n",
      "Dataset chargé: 3331 échantillons\n",
      "Chargement du dataset depuis sar_dataset.pkl...\n",
      "Dataset chargé: 3331 échantillons\n",
      "Division du dataset en ensembles d'entraînement/validation/test...\n",
      "Chargement du dataset depuis sar_dataset.pkl...\n",
      "Dataset chargé: 3331 échantillons\n",
      "Dataset chargé avec 3331 échantillons\n",
      "Répartition: 2331 entraînement, 499 validation, 501 test\n",
      "\n",
      "Configuration et entraînement du modèle CNN...\n",
      "Utilisation de l'appareil: cpu\n",
      "Modèle créé avec 33648610 paramètres\n",
      "Entraînement sur cpu pendant 30 époques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Époque 1/30 [Train]:  11%|█         | 8/73 [00:46<06:13,  5.75s/it, loss=nan, acc=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 138\u001b[0m\n\u001b[0;32m    134\u001b[0m     model\u001b[38;5;241m.\u001b[39mvisualize_results(metrics)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history, metrics\n\u001b[1;32m--> 138\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 115\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    112\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUtilisation de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mappareil: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 115\u001b[0m model, history, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_flood_detection_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# 4. Sauvegarder le modèle final\u001b[39;00m\n\u001b[0;32m    122\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_save_path)\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\flood_dataset\\flood-detection-cnn\\src\\models\\cnn.py:506\u001b[0m, in \u001b[0;36mtrain_flood_detection_model\u001b[1;34m(train_loader, val_loader, test_loader, model_params, training_params)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModèle créé avec \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m paramètres\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Évaluer le modèle si des données de test sont fournies\u001b[39;00m\n\u001b[0;32m    509\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\flood_dataset\\flood-detection-cnn\\src\\models\\cnn.py:150\u001b[0m, in \u001b[0;36mFloodDetectionCNN.train_model\u001b[1;34m(self, train_loader, val_loader, num_epochs, learning_rate, weight_decay, checkpoint_dir, early_stopping_patience)\u001b[0m\n\u001b[0;32m    147\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Passe avant\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Rétropropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\flood_dataset\\flood-detection-cnn\\src\\models\\cnn.py:81\u001b[0m, in \u001b[0;36mFloodDetectionCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03mPasse avant à travers le réseau\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    Tensor: Logits de sortie de forme [batch_size, num_classes]\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Extraction des caractéristiques\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Classification\u001b[39;00m\n\u001b[0;32m     84\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(features)\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\env\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\env\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mokht\\Desktop\\PDS\\env\\Lib\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    print(\"Démarrage de l'entraînement du modèle de détection d'inondations...\")\n",
    "    \n",
    "    # Définir les chemins\n",
    "    dataset_path = \"sar_dataset.pkl\"\n",
    "    checkpoints_dir = \"checkpoints\"\n",
    "    model_save_path = \"flood_detection_model.pth\"\n",
    "    \n",
    "    # Créer le répertoire de checkpoints s'il n'existe pas\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Utiliser la méthode create_dataloader pour charger les données\n",
    "    # Si le dataset existe déjà\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\"Chargement du dataset depuis {dataset_path}...\")\n",
    "        \n",
    "        # Option 1: Utiliser directement les dataloaders pour train/val\n",
    "        train_loader, train_dataset = SARDataset.create_dataloader(\n",
    "            use_saved_dataset=True,\n",
    "            saved_dataset_path=dataset_path,\n",
    "            batch_size=32,\n",
    "            num_workers=4,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader, val_dataset = SARDataset.create_dataloader(\n",
    "            use_saved_dataset=True,\n",
    "            saved_dataset_path=dataset_path,\n",
    "            batch_size=32,\n",
    "            num_workers=4,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        test_loader, test_dataset = SARDataset.create_dataloader(\n",
    "            use_saved_dataset=True,\n",
    "            saved_dataset_path=dataset_path,\n",
    "            batch_size=32,\n",
    "            num_workers=4,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Option 2: Alternative - Charger le dataset complet puis le diviser\n",
    "        # Ce qui permet une meilleure séparation train/val/test\n",
    "        print(\"Division du dataset en ensembles d'entraînement/validation/test...\")\n",
    "        dataset = SARDataset.load(dataset_path)\n",
    "        print(f\"Dataset chargé avec {len(dataset)} échantillons\")\n",
    "        \n",
    "        # 2. Diviser le dataset en train/val/test\n",
    "        train_size = int(0.7 * len(dataset))\n",
    "        val_size = int(0.15 * len(dataset))\n",
    "        test_size = len(dataset) - train_size - val_size\n",
    "        \n",
    "        # Utiliser une graine fixe pour la reproductibilité\n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "        train_dataset, val_dataset, test_dataset = random_split(\n",
    "            dataset, [train_size, val_size, test_size], generator=generator\n",
    "        )\n",
    "        \n",
    "        print(f\"Répartition: {train_size} entraînement, {val_size} validation, {test_size} test\")\n",
    "        \n",
    "        # Créer les dataloaders à partir des sous-ensembles\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        # Si le dataset n'existe pas encore, il faut le créer à partir du CSV\n",
    "        print(f\"Création du dataset à partir du CSV...\")\n",
    "        train_loader, train_dataset = SARDataset.create_dataloader(\n",
    "            batch_size=32, \n",
    "            num_workers=4,\n",
    "            shuffle=True,\n",
    "            save_after_loading=True,\n",
    "            saved_dataset_path=dataset_path\n",
    "        )\n",
    "        \n",
    "        # Dans ce cas, nous utilisons uniquement train_loader pour entraîner un modèle initial\n",
    "        # Idéalement, nous voudrions quand même diviser les données\n",
    "        print(\"ATTENTION: Vous devriez exécuter ce script à nouveau après création du dataset\")\n",
    "        print(\"pour pouvoir le diviser correctement en ensembles train/val/test.\")\n",
    "        \n",
    "        # Pour l'exemple, nous utilisons le même loader pour val et test (ce n'est pas optimal)\n",
    "        val_loader = train_loader\n",
    "        test_loader = train_loader\n",
    "    \n",
    "    # 3. Configurer et entraîner le modèle CNN\n",
    "    print(\"\\nConfiguration et entraînement du modèle CNN...\")\n",
    "    \n",
    "    # Paramètres du modèle\n",
    "    model_params = {\n",
    "        'input_channels': 2,  # VH et VV\n",
    "        'num_classes': 2,     # Inondé ou non inondé\n",
    "        'dropout_rate': 0.5   # Pour réduire le surapprentissage\n",
    "    }\n",
    "    \n",
    "    # Paramètres d'entraînement\n",
    "    training_params = {\n",
    "        'num_epochs': 30,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-5,\n",
    "        'checkpoint_dir': checkpoints_dir,\n",
    "        'early_stopping_patience': 5\n",
    "    }\n",
    "    \n",
    "    # Entraîner le modèle avec la fonction utilitaire\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Utilisation de l'appareil: {device}\")\n",
    "    \n",
    "    model, history, metrics = train_flood_detection_model(\n",
    "        train_loader, val_loader, test_loader,\n",
    "        model_params=model_params,\n",
    "        training_params=training_params\n",
    "    )\n",
    "    \n",
    "    # 4. Sauvegarder le modèle final\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Modèle sauvegardé à {model_save_path}\")\n",
    "    \n",
    "    # 5. Afficher un résumé des résultats\n",
    "    print(\"\\nRésumé des performances du modèle:\")\n",
    "    print(f\"Exactitude: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Précision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Rappel: {metrics['recall']:.4f}\")\n",
    "    print(f\"Score F1: {metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Visualiser les résultats\n",
    "    model.visualize_training_history(history)\n",
    "    model.visualize_results(metrics)\n",
    "    \n",
    "    return model, history, metrics\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = Dataset(split='train')\n",
    "val_dataset = Dataset(split='val')\n",
    "\n",
    "# Data augmentation\n",
    "train_dataset = augment_data(train_dataset)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=training_config['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = FloodDetectionCNN(**model_config)\n",
    "model.to(device)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(model, train_loader, val_loader, training_config)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "val_metrics = calculate_metrics(trainer.model, val_loader, device)\n",
    "print(val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully trained a CNN for flood detection using SAR images. The model's performance has been evaluated on the validation set, and metrics have been reported."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
